## ğŸ§ª Ù…Ø«Ø§Ù„â€ŒÙ‡Ø§ÛŒÛŒ Ø§Ø² Ø§Ø³ØªÙØ§Ø¯Ù‡ ØªØ§Ø¨Ø¹ `cleanize_text`

ØªØ§Ø¨Ø¹ `cleanize_text` Ø¨Ø±Ø§ÛŒ ØªÙ…ÛŒØ²Ø³Ø§Ø²ÛŒ Ù…ØªÙˆÙ† ÙØ§Ø±Ø³ÛŒ (Ø´Ø¹Ø±ØŒ Ú¯ÙØªØ§Ø±ØŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù†ÙˆÛŒØ²Ø¯Ø§Ø±) Ø·Ø±Ø§Ø­ÛŒ Ø´Ø¯Ù‡. Ø®Ø±ÙˆØ¬ÛŒ Ø´Ø§Ù…Ù„ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ ØªÙ…ÛŒØ² Ùˆ Ù…ØªÙ† Ù†Ù‡Ø§ÛŒÛŒ Ø§Ø³Øª.

### ğŸ”¹ ÙˆØ±ÙˆØ¯ÛŒ Ø¨Ù‡ ØµÙˆØ±Øª Ø±Ø´ØªÙ‡

```python
from cleanize_text import cleanize_text

text = "Ù…ÙÙ†â€Œ Ø§Ø² Ø¢Ù† Ø±ÙˆØ² Ú©Ù‡ Ø¯Ø± Ø¨Ù†Ø¯Ù ØªÙˆØ§Ù… Ø¢Ø²Ø§Ø¯Ù…Ø› Û±Û²Û³Û´Ûµ!"
tokens, cleaned = cleanize_text(text)

print("Tokens:", tokens)
print("Cleaned Text:", cleaned)
- Tokens: ['Ù…Ù†', 'Ø§Ø²', 'Ø¢Ù†', 'Ø±ÙˆØ²', 'Ú©Ù‡', 'Ø¯Ø±', 'Ø¨Ù†Ø¯', 'ØªÙˆØ§Ù…', 'Ø¢Ø²Ø§Ø¯Ù…']
- Cleaned Text: Ù…Ù† Ø§Ø² Ø¢Ù† Ø±ÙˆØ² Ú©Ù‡ Ø¯Ø± Ø¨Ù†Ø¯ ØªÙˆØ§Ù… Ø¢Ø²Ø§Ø¯Ù…



tokens_input = ['Ø®ÙˆØ§Ø¨', 'Ø¯ÛŒØ¯Ù…ØŒ', 'Ú©Ù‡', 'Ø¨Ø§Ø±Ø§Ù†', 'Ù…ÛŒâ€ŒØ¨Ø§Ø±ÛŒØ¯!', 'Û±Û²Û³']
tokens, cleaned = cleanize_text(tokens_input)

print("Tokens:", tokens)
print("Cleaned Text:", cleaned)

- Tokens: ['Ø®ÙˆØ§Ø¨', 'Ø¯ÛŒØ¯Ù…', 'Ú©Ù‡', 'Ø¨Ø§Ø±Ø§Ù†', 'Ù…ÛŒâ€ŒØ¨Ø§Ø±ÛŒØ¯']
- Cleaned Text: Ø®ÙˆØ§Ø¨ Ø¯ÛŒØ¯Ù… Ú©Ù‡ Ø¨Ø§Ø±Ø§Ù† Ù…ÛŒâ€ŒØ¨Ø§Ø±ÛŒØ¯



noisy_input = [None, 'Ú©Ø¬Ø§ØŸ', 42, 'Ø¨Ù‡Ø§Ø±']
tokens, cleaned = cleanize_text(noisy_input)

print("Tokens:", tokens)
print("Cleaned Text:", cleaned)

- Tokens: ['Ú©Ø¬Ø§', 'Ø¨Ù‡Ø§Ø±']
- Cleaned Text: Ú©Ø¬Ø§ Ø¨Ù‡Ø§Ø±



